#  Mutiple linear regression on fish market dataset

#  ### Project Overview
In this project, we aim to build a multiple linear regression model to predict the weight of fish based on various features. The dataset used is `Fish.csv` fish market dataset from Kaggle Datasets

# 1. Initial setup 
# 1.1 Uploding dataset to google notebook

from google.colab import files
files.upload()


#  1.2 Checking for Dateset availability 

!ls


# 2. Preprocessing Data


   2.1 Getting Preview of Dataset

import pandas as pd
data = pd.read_csv('Fish.csv')
data.head()

##  We have a categorical feature in 'Species" column and There are not any missing value in this data set so we don't need missing data handeling.

## 2.2 Encoding Categorical Variables:
 Convert categorical variables into dummies variables using get_dummies methode For avoding muticollinearity error. We remove one culumn dummy by passing 'drop_first = True' to 'get_dummies() method.

data = pd.get_dummies(data, columns=['Species'], drop_first=True)
data.head()


#  2.3 Search  Correlation between features


import seaborn as sns
import matplotlib.pyplot as plt
corr_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

# 2.3 Explain: 
In this Dataset we have strong correlation coeficient between 'length1','length2' and ''length3'(bigger than 0.8) so my approch is combining these 3 featurs using mean method


# 2.4 Combining  'length1','length2' and ''length3' with using Mean method

data['average_len'] = data[['Length1' ,'Length2','Length3']].mean(axis=1)
data.head()

# 2.4.1 We drop 3 columns Length1	Length2	Length3

data = data.drop(columns = ['Length1','Length2','Length3'])
data.head()

#  2.4.2 We review for being all correlation coefficients less than 0.8

import seaborn as sns
corr_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

# 2.4.2 Explain:
Now All old features have correlations less than 0.8 with each others but we have correlation between width and our new feature (average_len) it is equal 0.87 so I need disappear this added correlation to my dataset before doing every job .best predictors are average_len=0.92, Width=0.89 , Heigth=0.72 (highest correlation with our Target 'Weight')

# 2.4.2.1 Combining 'width' and 'average_len' with using Mean method

data['ave_len_width'] = data[['average_len' ,'Width']].mean(axis=1)
data = data.drop(columns = ['Width' , 'average_len'])
data.head()

# 2.4.2.2 Recheck corelations between features

corr_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

# 2.4.2.2 Explain: 
Now all correlations in our dataset is less than 0.8 and best predictors are ave_len_width=0.93, Heigth=0.72 (highest correlation with our Target 'Weight')

#  2.5 avoiding multicollinearity by using IVF
  For avoiding multicollinearity it is necessary we drop all features that they have IVF>5 so we recheck in second way by VIF 

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assuming 'data' is your DataFrame and 'Weight' is your target variable
X = data.drop(columns=['Weight'])  # Drop the target variable

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print(vif_data)



# 2.5 Explain Error: 
after using dummi variable we have boolian data type in our data set(True/False value) and it causing issues with the VIF calculation(method accept numeric values). for correcting this issue I convert all dataset values to numeric values.(True =>1.0 /False =>0.0)

# 2.5.1 Error disappear process



data= data.astype(float)
data.head()

# 2.5.2 Now we can calcuate VIF

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assuming 'data' is your DataFrame and 'Weight' is your target variable
X = data.drop(columns=['Weight'])  # Drop the target variable

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print(vif_data)

# 2.5.2 Interperet result:
We see VIF is 74.63 for Height and 11.14 for Species_Pike and 128.8 for ave_len_width (VIF>10 =>serious collinearity) so I use Dimensionality Reduction techniques in this case Principal Component Analysis (PCA) to transform Height , ave_len_width and Species_Pike into new uncorrelated variables.


# 2.5.3 Using PCA method for disappearing cases with IVF>5

from sklearn.decomposition import PCA
#import pandas as pd

# Assuming 'data' is your DataFrame and 'Weight' is your target variable
X = data.drop(columns=['Weight'])  # Drop the target variable

# Apply PCA
pca = PCA(n_components=3)  # Adjust 'n_components' as needed
X_pca = pca.fit_transform(X[['Height', 'ave_len_width', 'Species_Pike']])

# Create a new DataFrame with the PCA components
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])
X = X.drop(columns=['Height', 'ave_len_width','Species_Pike'])  # Drop original features
X = pd.concat([X, pca_df], axis=1)  # Add PCA components
X_1 =X.copy()
X.head()


# 2.5.4 Recheck VIF values 
for each features in our data set

vif_data["feature"] = X_1.columns
vif_data["VIF"] = [variance_inflation_factor(X_1.values, i) for i in range(len(X_1.columns))]
vif_data

#  Ok that's good result. all VIFs are less than 5. we can go to next step 

# 3 Model Training

#  3.1 Split my dataset into training and testing sets:

from sklearn.model_selection import train_test_split

X = X_1  # Your features after PCA
y = data['Weight']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)


#  3.2 Train my multiple linear regression model:

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

#  4 Model Evaluation:

Evaluate my model using metrics like R², Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE):

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)
print("R²:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))


# 5 getting solid insight on Our Model's Predictions


5.1 Access Coefficients:

coefficients = model.coef_
feature_names = X_1.columns

coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
coef_df


# 5.2 Checking the Intercept (Bias Term):

intercept = model.intercept_
print("Intercept:", intercept)


# 5.3 Understanding the Multiple Linear Regression Model: 
 We calculate the coefficients of the features and the intercept. The model prediction is based on this equation:

Weight = (42.87*Species_Parkki) + (-87.56*Species_Perch) + (-94.94*Species_Roach) + (157.98*Species_Smelt) + (-78.31*Species_Whitefish) + (49.51*PC1) + (-19.68*PC2) + (-440.84*PC3) + 421.285
