{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-W9su29AqD_"
      },
      "outputs": [],
      "source": [
        "#  Mutiple linear regression on fish market dataset\n",
        "#  ### Project Overview\n",
        "In this project, we aim to build a multiple linear regression model to predict the weight of fish based on various features. The dataset used is `Fish.csv` fish market dataset from Kaggle Datasets\n",
        "# 1. Initial setup\n",
        "# 1.1 Uploding dataset to google notebook\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "#  1.2 Checking for Dateset availability\n",
        "!ls\n",
        "# 2. Preprocessing Data\n",
        "   2.1 Getting Preview of Dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('Fish.csv')\n",
        "data.head()\n",
        "##  We have a categorical feature in 'Species\" column and There are not any missing value in this data set so we don't need missing data handeling.\n",
        "## 2.2 Encoding Categorical Variables:\n",
        " Convert categorical variables into dummies variables using get_dummies methode For avoding muticollinearity error. We remove one culumn dummy by passing 'drop_first = True' to 'get_dummies() method.\n",
        "data = pd.get_dummies(data, columns=['Species'], drop_first=True)\n",
        "data.head()\n",
        "#  2.3 Search  Correlation between features\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "corr_matrix = data.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "# 2.3 Explain:\n",
        "In this Dataset we have strong correlation coeficient between 'length1','length2' and ''length3'(bigger than 0.8) so my approch is combining these 3 featurs using mean method\n",
        "# 2.4 Combining  'length1','length2' and ''length3' with using Mean method\n",
        "data['average_len'] = data[['Length1' ,'Length2','Length3']].mean(axis=1)\n",
        "data.head()\n",
        "# 2.4.1 We drop 3 columns Length1\tLength2\tLength3\n",
        "data = data.drop(columns = ['Length1','Length2','Length3'])\n",
        "data.head()\n",
        "#  2.4.2 We review for being all correlation coefficients less than 0.8\n",
        "import seaborn as sns\n",
        "corr_matrix = data.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "# 2.4.2 Explain:\n",
        "Now All old features have correlations less than 0.8 with each others but we have correlation between width and our new feature (average_len) it is equal 0.87 so I need disappear this added correlation to my dataset before doing every job .best predictors are average_len=0.92, Width=0.89 , Heigth=0.72 (highest correlation with our Target 'Weight')\n",
        "# 2.4.2.1 Combining 'width' and 'average_len' with using Mean method\n",
        "data['ave_len_width'] = data[['average_len' ,'Width']].mean(axis=1)\n",
        "data = data.drop(columns = ['Width' , 'average_len'])\n",
        "data.head()\n",
        "# 2.4.2.2 Recheck corelations between features\n",
        "corr_matrix = data.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "# 2.4.2.2 Explain:\n",
        "Now all correlations in our dataset is less than 0.8 and best predictors are ave_len_width=0.93, Heigth=0.72 (highest correlation with our Target 'Weight')\n",
        "#  2.5 avoiding multicollinearity by using IVF\n",
        "  For avoiding multicollinearity it is necessary we drop all features that they have IVF>5 so we recheck in second way by VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# Assuming 'data' is your DataFrame and 'Weight' is your target variable\n",
        "X = data.drop(columns=['Weight'])  # Drop the target variable\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "print(vif_data)\n",
        "# 2.5 Explain Error:\n",
        "after using dummi variable we have boolian data type in our data set(True/False value) and it causing issues with the VIF calculation(method accept numeric values). for correcting this issue I convert all dataset values to numeric values.(True =>1.0 /False =>0.0)\n",
        "# 2.5.1 Error disappear process\n",
        "data= data.astype(float)\n",
        "data.head()\n",
        "# 2.5.2 Now we can calcuate VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# Assuming 'data' is your DataFrame and 'Weight' is your target variable\n",
        "X = data.drop(columns=['Weight'])  # Drop the target variable\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "print(vif_data)\n",
        "# 2.5.2 Interperet result:\n",
        "We see VIF is 74.63 for Height and 11.14 for Species_Pike and 128.8 for ave_len_width (VIF>10 =>serious collinearity) so I use Dimensionality Reduction techniques in this case Principal Component Analysis (PCA) to transform Height , ave_len_width and Species_Pike into new uncorrelated variables.\n",
        "# 2.5.3 Using PCA method for disappearing cases with IVF>5\n",
        "from sklearn.decomposition import PCA\n",
        "#import pandas as pd\n",
        "# Assuming 'data' is your DataFrame and 'Weight' is your target variable\n",
        "X = data.drop(columns=['Weight'])  # Drop the target variable\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=3)  # Adjust 'n_components' as needed\n",
        "X_pca = pca.fit_transform(X[['Height', 'ave_len_width', 'Species_Pike']])\n",
        "# Create a new DataFrame with the PCA components\n",
        "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])\n",
        "X = X.drop(columns=['Height', 'ave_len_width','Species_Pike'])  # Drop original features\n",
        "X = pd.concat([X, pca_df], axis=1)  # Add PCA components\n",
        "X_1 =X.copy()\n",
        "X.head()\n",
        "# 2.5.4 Recheck VIF values\n",
        "for each features in our data set\n",
        "vif_data[\"feature\"] = X_1.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_1.values, i) for i in range(len(X_1.columns))]\n",
        "vif_data\n",
        "#  Ok that's good result. all VIFs are less than 5. we can go to next step\n",
        "# 3 Model Training\n",
        "#  3.1 Split my dataset into training and testing sets:\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = X_1  # Your features after PCA\n",
        "y = data['Weight']  # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "#  3.2 Train my multiple linear regression model:\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "#  4 Model Evaluation:\n",
        "Evaluate my model using metrics like R², Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE):\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"R²:\", r2_score(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "# 5 getting solid insight on Our Model's Predictions\n",
        "5.1 Access Coefficients:\n",
        "coefficients = model.coef_\n",
        "feature_names = X_1.columns\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "coef_df\n",
        "# 5.2 Checking the Intercept (Bias Term):\n",
        "intercept = model.intercept_\n",
        "print(\"Intercept:\", intercept)\n",
        "# 5.3 Understanding the Multiple Linear Regression Model:\n",
        " We calculate the coefficients of the features and the intercept. The model prediction is based on this equation:\n",
        "Weight = (42.87*Species_Parkki) + (-87.56*Species_Perch) + (-94.94*Species_Roach) + (157.98*Species_Smelt) + (-78.31*Species_Whitefish) + (49.51*PC1) + (-19.68*PC2) + (-440.84*PC3) + 421.285"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## License This project is licensed under the MIT License - see the LICENSE file for details.\n",
        "## © 2024 Ali M Shafiei. All rights"
      ],
      "metadata": {
        "id": "Hh0GRtTbA7I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B5mwYjB6A7HA"
      }
    }
  ]
}